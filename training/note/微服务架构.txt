# 微服务架构

## 概览

微服务架构的复杂源于网络间通信
微服务框架就是要解决这种架构下, 组件之间的发现, 通信, 容错等问题

微服务拆分方式: 按照组织架构拆分; 按照DDD的限界上下文拆分
微服务合适的粒度: 三四个人能清楚服务的细节, 即两个熟悉项目的人带两个新手

微服务框架主要问题 http://mc.xchch.top:6670/download/tech/2211/微服务框架主要问题.png
通信: 服务之间如何发起调用, 一般就是rpc, 或者是HTTP直接通信
服务治理: 涵盖从服务注册与发现到可观测性的全部内容

在底层通信协议上, 遇事不决用gRPC, 如果是小型系统, 可以考虑直接使用HTTP接口
gRPC比较学院派, 它是典型的使用IDL来生成代码的RPC框架. 是多语言通信的首选
IDL(interface description/definition language): 接口描述语言/接口定义语言. 是指用一种中间语言来定义接口, 而后为其他的语言生成对应代码的设计方案
gRPC使用的IDL是protobuf
protobuf也定义了序列化格式, 所以我们也常说使用protobuf来作为序列化协议

kratos

面试要点
微服务框架是什么? 主要就是解决两个问题, 通信和服务治理
为什么使用微服务框架? 本质上是为了分而治之, 将业务拆分之后独立治理, 部署
RPC框架和RestFul框架有什么区别? 应该说, 两者基本没联系, 全是区别. 唯一的联系, 就是RPC框架可以利用RestFul来实现.
    RestFul是指符合REST风格的HTTP接口, 而RPC指的是通过远程调用, 本质上就是两回事
RPC框架和Web框架有什么区别? 基本没什么联系, 都是区别. 唯一的共同点可以通过对web框架封装来实现RPC通信

## RPC

代码演示 rpc_v1
v0: 反射篡改方法的调用
v1: 单元测试增加返回值的校验
v2: 代码中实现response部分
v3: 单元测试代码重构
v4: 单元测试增加异常场景的用例
v5: 增加e2e测试代码
v6: 服务端处理rpc请求
v7: 客户端读取rpc的响应结果
v8: 使用reflectionStub对server代码重构

rpc执行流程总结
客户端
1. 首先反射拿到request, 核心是服务名字, 方法名字和参数
2. 将request进行编码, 要注意序列化并且加上长度字段
3. 使用连接池, 或者一个连接, 把请求发出去
4. 从连接里面读取响应, 解析成结构体
服务端
1. 启动一个服务端, 监听一个端口
2. 读取长度字段, 再根据长度, 读完整个消息
3. 解析成request
4. 查找服务, 和对应的方法
5. 构造方法对应的输入
6. 反射执行调用
7. 编码响应
8. 写回响应

面试要点
什么是rpc? 远程过程调用. 类似的还有RMI, 远程方法调用
和使用http接口比起来, 使用RPC有什么优势? 不必关心http调用的细节, 对于使用者来说就如同本地调用一般
rpc框架的要点是什么? 客户端捕捉调用信息, 编码成二进制, 发送到服务端. 服务端查找本地服务, 执行调用, 写回响应. 任何一个rpc框架都类似
rpc框架怎么捕捉本地调用信息? 主要依赖于代理模式和代码生成模式
什么是代理模式? 什么是动态代理模式? 动态代理模式可以看做是动态生成的代理, 一般指运行时生成的代理
动态代理技术能用来做什么? 四个字, 为所欲为. 在这里就是用来发起rpc调用, 然后再返回响应

## grpc协议

grpc协议分为头部和body两个部分, 因为grpc是直接基于http来实现的, 所以, grpc的头部就是放在http协议头, body就是放在http协议体里面
grpc和dubbo协议都可以看做是应用层协议, 其他协议设计也是类似的, 比如tcp协议

协议设计总结
协议一般都是分成两个部分: 协议头和协议体
协议头包含接收方"如何处理这个消息"的必要信息, 具体来说就是描述协议本身的数据, 和描述这次请求的数据
协议体则大多数情况下都是存放请求数据

从形态上来说, 协议体必然是变长的; 而协议头可以是定长的, 也可以是变长的.
在变长协议头的情况下, 需要有两个字段来描述长度, 一个描述协议头有多长, 一个描述协议体有多长

代码演示 rpc_v2
v0: 上一版本的最终代码
v1: 定义request和response的的协议头
v2: 增加encodeReq和decodeReq的测试, 完成e2e测试
v3: 对消息体进行编解码, 完成e2e测试
v4: 对消息体使用protobuf编解码, 完成e2e测试

面试要点
RPC协议主要包含什么? 消息头和消息体
头部包含什么? 有什么用?
RPC里面为什么包含长度字段? 主要是为了切割消息, 也就是所谓的粘包问题
头部可以是变长的吗? 可以
大概描述一下gRCP协议? 关键点gRPC利用了HTTP协议, 然后再描述一下gRCP的几个常见头部
为什么尽量把元数据之类的东西放在消息头? 主要是考虑到sidecar网关之类的东西, 它们可以做到解析部分数据, 提高性能
如何在RPC协议里面支持不同序列化协议/压缩算法? 但凡问道如何在RPC协议上支持xx功能, 核心都是要在协议本身加上对应的字段.
    要考虑放到协议体或者协议头, 然后客户端和服务端都要做响应的修改

## 单向调用

rpc调用语义
异步调用: 指用户发起调用之后, 就可以去做别的事情, 而后在一段时间后再处理
单向调用(one-way): 是指用户发起调用之后, 不需要结果
回调: 用户在发起调用的时候注册一个回调, 当结果返回来之后, 用户会继续

单向调用分为真假两种
虚假的单向调用: 是指用户的请求发过去之后, 服务端会把响应发过来. 但是客户端收到响应之后会直接丢弃
真实的单向调用: 是指用户的请求发过去之后, 服务端发现是一个单向调用, 直接就不会发响应回来, 读完请求之后, 连接等资源就释放了
    核心在于: 单向调用一定是为了尽快释放两端资源的, 因此虚假的单向调用, 毫无意义

代码演示 rpc_v3
v0: 上一版本最终代码
v1: 增加oneway的代码实现, e2e测试没有通过

面试要点
什么是异步调用? go里面怎么实现? 在有goroutine的情况下, 框架设计者没有必要支持
什么是回调? go里面怎么实现? 同上
什么是one-way(单向)调用? 要注意解释真伪两种one-way的形态, 最关键在于, 服务端究竟会不会回写响应, 而后进一步讨论两种形态对于性能的影响
使用one-way有什么有点? 性能, 尽早释放资源

## 超时控制

在微服务里面, 超时控制分成两种: 单一服务超时控制和链路超时控制. 绝大多数微服务框架的超时控制都只支持单一服务超时控制
设置整个链路的超时时间, 可能是web, 可能是BFF

链路超时控制总结
1. rpc客户端需要监听cxt, 并且要把剩余超时时间传递给rpc服务端
2. rpc服务端收到请求后, 要检测元数据里面有没有携带剩余超时时间, 然后重建context.Context
3. 如果用户的业务里面用到了其他中间件, 那么用户可能需要自己手动管理超时元数据, 继续传递给其他中间件
4. 任何一个环节的超时时间, 都不可能超过链路超时时间

监听超时 http://mc.xchch.top:6670/download/tech/2211/监听超时.png
1. 理论上来讲, 超时监听可以: 只在客户端监听 或者 客户端和服务端同时监听
2. 但是不能只在服务端监听. 当服务端发现超时, 而后返回超时响应的时候, 如果网络故障, 那么客户端收不到超时响应, 因此会一直在那里等

代码演示 rpc_v4
v0: rpc_v2 v4的代码
v1: 增加超时检测的实现

完成了本地的超时控制, 现在我们需要将链路超时时间传递下去给服务端. 类似于前面的单向调用, 也是在原数据部分, 带一个key-value过去
问题? 超时时间传什么内容
1. 传递剩余超时时间, 类似于1s, 3s这种    缺点: 难以估计网络传输时间
2. 传递超时时间戳, 类似于某时某刻过期     缺点: 时间同步问题
总结: 两个方案之间, 没有特别强的优劣之分

面试要点
rpc怎么控制超时? 客户端控制, 服务端控制的特点
为什么要使用超时控制? 及时释放资源
超时时间没有设置好, 会有什么问题? 过短--大部分请求超时, 过长--浪费资源, 甚至引起goroutine
超时时间在链路中传递, 传递的是什么? 剩余超时时间或者超时时间戳. 进一步可以讨论时间戳与时间同步的问题
超时之后可以终端业务执行吗? 不可以
链路超时怎么实现? 核心就是链路中传递超时时间, 这个主要是依赖于rpc协议里传递超时时间.
    注意, 如果你是一个中间件设计者, 你还要考虑用户可能希望重置整个链路的超时时间, 那么你要设计类似的接口

## 服务注册与发现

服务注册与发现演进
第一阶段: 直接IP+端口访问  特点: 客户端保存着IP端口信息, 需要提前配置好
    缺点: IP地址会变, 如果实例很多, 配置难以维护
第二阶段: 域名解析  特点: 客户端保存服务的endpoint, 客户端也可以缓存dns解析的结果
    缺点: 客户端不缓存则多一次调用, 缓存则存在不一致性
第三阶段: 分布式协调-注册中心阶段 特点: 服务端发起注册, 客户端向注册中心询问, 注册中心维持住两端
    缺点: 基本没有显著的缺点, 不过在大规模集群下, 注册中心容易称为瓶颈, 网络中比较多探活流量

总结
服务注册中心模式, 核心是依赖于一个第三方组件
基本模型
1. 服务启动成功之后主动注册
2. 服务端和注册中心保持心跳
3. 客户端启动的时候要主动订阅对应服务的数据
4. 注册中心要通知客户端变更
理解难点
1. 运行过程中, 客户端连不上注册中心怎么办?
2. 运行过程中, 客户端拿到了注册数据, 但是连不上对应的服务端怎么办?
3. 注册过程中, 注册中心没有收到服务端心跳怎么办?
4. 注册中心崩溃了, 客户端和服务端怎么办?

gRPC服务注册与发现
grpc服务注册与发现的核心在resolver包, 核心接口有
1) Target: 被解析的目标, 是对服务的抽象
2) Builder: Builder模式, 用于构建一个Resolver
Resolver: 负责服务发现
gRPC服务注册与发现的重要实现就是基于DNS的实现: dnsResolver和dnsBuilder
其中核心方法时watcher, 它的整体逻辑可以看成是一个拉模型, 即轮询DBS服务器来刷新本地可用的服务器列表
服务发现里面的两个基本模型
1) 推模型: 注册中心主动推送变更给客户端
2) 拉模型: 客户端轮询注册中心. 这种模型其实不好管, 因为用户(或者中间件研发者)要考虑多久刷新一次, 快了浪费资源, 慢了又是数据不一致

Kratos服务注册与发现
Kratos本身是建立在gRPC上的, 但是它又有自己的服务注册与发现接口
1) Register: 对应于服务注册部分
2) Discovery: 对应于服务发现部分
3) Watcher: 监听服务变更部分

服务注册与发现总结
注册一般就是两种维度
1) 接口维度: 一个接口注册一次, 服务发现的时候也是使用接口来查找
2) 应用维度: 一个应用注册一次, 一个应用有很多服务. 这些服务共享一些基本信息, 同时不同服务还有独特的信息
两种注册维度又是不是很明显, 并且两种都有人用: 接口维度可控性更强, 粒度更细, 但是写入的数据, 心跳等开销更大; 应用维度粒度更细, 但是数据更少
注册数据则总体分为两类
定位信息: 例如 ip+端口
其他: 这主要取决于微服务框架的具体功能, 例如在Dubbo-go里面可以写入标签信息, 分组信息
核心接口 ①服务注册--针对服务端 ②服务发现--针对客户端 ③监听变更--针对客户端

## 自定义服务注册与发现

目标: 使用etcd来实现一个服务注册中心, 在gRPC内部接入我们自己的服务注册与发现
gRPC服务注册与发现机制
1. gRPC本身没有提供服务注册接口, 也就是服务注册本身, 是我们自己管的
2. gRPC只提供了服务解析的接口, 也就是Resolver和对应的Builder两个接口

代码演示 micro_v1
v0: 完成服务注册v0代码
v1: ResolverNow方法和watch方法单元测试
v2: 完成etcd实现服务注册的功能, 完成e2e测试代码

mock代码生成
mockgen -package mocks -destination mocks/kv.mock.go -source C:\Users\Administrator\go\pkg\mod\go.etcd.io\etcd\client\v3@v3.5.4\kv.go
mockgen -package mocks -destination mocks/watch.mock.go -source C:\Users\Administrator\go\pkg\mod\go.etcd.io\etcd\client\v3@v3.5.4\watch.go

gRPC服务发现流程
1. 用户在初始化gRPC的时候指定grpc.WithResolver选项, 传入自定义的Resolver
2. 在Dial调用的时候传入服务标识符, 一般形式是scheme:///service-name, scheme代表的是如何通信的. 大多数时候, 它就是代表我们注册中心
3. gRPC会根据scheme来找到我们注册的Resolver, 我们在Resolver里面更新可用的连接

服务崩溃之后, 客户端多久才能知道
取决于服务端和注册中心的交互方式:
    如果是注册中心主动发起心跳, 那么就主要取决于心跳的间隔, 以及多少次心跳失败才会判定服务器崩溃
    如果是服务端主动续约而没有心跳, 那么就取决于租约长短, 以及续约的重试机制
心跳是一个很复杂的事情:
    心跳频繁, 那么挤占正常请求的资源, 但是容易发现服务端崩溃
    心跳不频繁, 那么注册中心很难发现节点崩溃
心跳失败之后的判定
    一次心跳失败判定节点失活: 过于严苛, 部分时候网络抖动就会引起误判
    连续多次心跳判定节点失活: 连续多次失败才判定一个节点崩溃, 未能及时发现节点崩溃

客户端和注册中心连不上怎么办?
可能 ①网络问题 ②注册中心崩溃
策略
    客户端直接停止服务, 直到恢复和注册中心的连接
    客户端继续服务, 并且尝试重新脸上注册中心, 这段时间内使用的都是本地缓存数据. 再过一段时间连不上之后, 再停止服务

利用健康检查来决定注册时机
微服务框架里面有一种做法, 就是微服务框架在启动服务之后, 会给服务发一个健康检查或者心跳, 如果收到了成功的响应, 那么就认为服务启动成功了
只有在这个时候, 微服务框架才会注册数据. 实际上这个东西没什么用, 因为你健康检查通过了, 只能认为你端口启动成功了, 你微服务在业务层面上启动了没有, 还是不知道的.

容器内部IP问题
注意到, 我们的etcd实现里面, 我们使用的是IP+端口来识别, 问题就在于如果微服务运行于容器内部, 例如Docker, 那么拿到的IP其实永远都是localhost
而客户端运行在另外一个容器内, 使用localhost是无法连接上服务端的
在这种情况下, 微服务只能选择:
    使用容器专属的注册与发现方式
    容器启动的时候使用宿主机的网络, 而不是创建一个虚拟网络
    容器启动的时候想办法把宿主机的IP作为环境变量注入进去

中间件选型
体量小的时候, 10000个服务实例内, 只要是主流的就都没问题: zookeeper, nacos, etcd
在体量大之后要考虑
    集群模型: 对等模型和主从模型
    CAP: 大集群偏向AP, 小集群偏向CP. AP存在客户端缓存的数据过期问题, CP存在注册中心不可用问题

## 负载均衡

注册中心解决的是有哪些可用服务实例, 负载均衡解决的是这么多可用服务实例, 我该把请求发给谁?
从理论上来说, 我们希望将请求发给那个能最快返回响应给我的实例

主流的负载均衡算法
完全不实时计算负载的算法: 轮询, 加权轮询, 随机, 加权随机, 哈希, 一致性哈希
尝试实时计算负载的算法: 最快响应时间, 最少连接数, 最少请求数算法

代码演示 micro_v2
v0: 负载均衡实现轮询算法
v1: 负载均衡实现加权轮询算法
v2: 负载均衡的最少活跃数算法实现 - 没有测试

总结
要不要考虑服务器处理能力? 轮询, 随机, 哈希, 最小连接数, 最少活跃数都没考虑
选择什么指标来表达服务器当前负载?
    随机, 轮询, 哈希 什么都没选, 依赖于统计; 连接数, 请求数, 响应时间, 错误数 这些你可以随便选几个指标, 然后设计自己的负载均衡算法
是不是所有的请求所需资源都是一样的? 显然不是
    大商家品类极多, 大买家订单极多; 不考虑请求消耗资源的负载均衡, 容易出现偶发的打爆某一台实例的情况

微服务框架的局限性 http://mc.xchch.top:6670/download/tech/2211/负载均衡选择-微服务框架局限性.png
客户端选择服务端作为服务提供者时, 是缺乏全局信息的, 那么为什么他们运作的还是很好呢?
因为请求数量多了, 慢慢就会收敛到一种比较均匀的状态

设计自己的负载均衡算法
核心是根据自己的服务特征来选取一些指标, 来表达实例的负载. 指标可以是服务指标(错误率等)和硬件指标(CPU、IO、网络负载等)

## 路由策略

负载均衡时的业务需求, 称为路由策略. 业务需求可以有
* 在A/B测试中, A请求只能发过去A节点
* 在全链路压测中, 压测流量只能发过去测试节点上
* 在VIP服务中, VIP的请求要发到更加高端的机器上
* 在联调或者DEBUG的时候, 请求只能发送到一个特定的机器上

代码演示 micro_v3
v0: micro_v2的v1版本代码
v1: 增加分组的功能
v2: 增加广播的功能

过滤功能对负载均衡算法的影响
首先从实现的角度来说, 大部分负载均衡算法都受到了影响, 包括随机、轮询, 以及对应的加权版本
过滤功能使用不当可能会造成负载均衡算法几乎失效
    过滤条件太苛刻以至于满足条件的实例几乎没有
    每次请求过滤之后的节点都不同, 那么可能导致所有的请求都发过去了少部分实例

自定义路由策略
路由是一个非常强业务相关的特性, 即大多数时候我们是根据业务规则来设计路由的, 但是实现和前面我们提到的分组是类似的
设计一个路由可以从以下几个方向考虑
1. 资源隔离角度: 例如VIP用户和普通用户隔离, 付费用户和免费用户隔离
2. 测试: 可以为测试设计专属的路由, 例如在全链路压测中
3. 动态分组: 例如在运行时刻根据节点状况打不同的标签

## 集群抽象 - Cluster

集群抽象我们在调用远程服务的时候, 尝试解决
* failover: 即引入重试功能, 但是重试的时候会换一个新节点
* failfast: 立即失败, 不许需要重试
* 广播: 将请求发送到所有的节点上
* 组播: 组播和分组功能不一样, 组播是指将请求发送到一组节点上, 而不是只发送到单一一个节点上

cluster、路由和负载均衡的关系
本质上, 它们都回答同一个问题: 我要把请求发给谁?
一般来说, cluster是不需要考虑负载均衡的 -- 无论是组播还是广播, 都是发给多个节点
但是cluster中的组播, 可以理解为广播+路由, 因为路由的本质就是筛选出节点

## 可用性

可用性在微服务框架里面是和服务治理最密切相关的主题了
包含熔断 限流 降级 重试 超时控制

熔断, 限流, 降级 并没有本质区别, 都可以归属到故障处理的范畴里面
第一阶段 故障检测: 使用一些特定的算法, 判定服务是否处于不健康状态
第二阶段 故障处理
    限流: 一段时间内只允许特定数量的请求被处理
    熔断: 全部请求都会被拒绝
    降级: 全部请求都会执行一段更加简单的逻辑
第三阶段 故障恢复: 即如果服务被判定为处于异常状态之后, 什么时候再恢复过来

故障检测算法, 分为两类, 对于绝大多数应用来说, 静态类型算法就足够了
静态类型的算法: 令牌桶, 漏桶, 固定窗口和滑动窗口都属于这一类
动态类型的算法: 根据错误率, 根据响应时间, 或者典型的BBR算法

代码演示 micro_v4
v0: micro_v2的v1版本代码